"""
@author: ieva-balta, majocava, naaattella
"""

import logging
import ssl
import pickle
import os
import pandas as pd
import requests
import time
from requests.adapters import HTTPAdapter
import urllib.parse

from urllib3 import Retry
from matplotlib_venn import venn2
import matplotlib.pyplot as plt

"""
This script retrieves the existence levels of the unique accessions code in the dbPTM/CPLM version of FLAMS when compared to the UniProt dataset.
Morever, it retrieves the PTM features, and whether those accession codes have been collapsed into UniProt canonical entries and retrieve the new accession

It requires two files: 

 1. PTM_old_organism.csv: Generated by running the 0.dataframe_building.py script. Contains a summary of all the accession codes in dbPTM/CPLM database from Zenodo (version 1.4)
 2. uniprot_organism_data_test.csv: Generated by running the 0.dataframe_building.py script. Contains a summary of all the accession codes in the UniProt dataset. 
 
"""


# Change the path to the files if needed
# Data from zenodo
ptm_df = pd.read_csv("PTM_old_organism.csv")
# Load the uniprot csv
uniprot_df = pd.read_csv("uniprot_organism_data_test.csv")

# Analyze accessions
old_accessions_iso = set(ptm_df['UniProt_Accession'].unique())
uniprot_accessions_iso = set(uniprot_df["UniProt_Accession"].unique())

print(F"Number of final proteins with isoforms (dbptm/cplm): {len(old_accessions_iso)}")
print(F"Number of final proteins with isoforms (uniprot): {len(uniprot_accessions_iso)}")


# Compute
total_unique_iso = old_accessions_iso.union(uniprot_accessions_iso)

# Compute intersection
common_iso = old_accessions_iso.intersection(uniprot_accessions_iso)

# PTMs unique in dbptm + CPLM
unique_old_iso = old_accessions_iso - uniprot_accessions_iso

# PTMs unique in uniprot
unique_uniprot_iso = uniprot_accessions_iso - old_accessions_iso

# Function to create a session with retries
def requests_session_with_retries():
    session = requests.Session()
    retry = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

# # Get the unique in dbptm/CPLM, the unique in uniprot and check whether those proteins are existing:1, or not
unique_old_list = list(unique_old_iso)
print(f"Number of accessions that are unique to old database: {len(unique_old_iso)}")

session = requests_session_with_retries()
records = []
batch_size = 10000
save_counter = 0
checkpoint_file = "checkpoint.pkl"

for i in range(0, len(unique_old_list), batch_size):
    batch = unique_old_list[i:i + batch_size]
    batch_num = i // batch_size + 1
    ids = ','.join(batch)
    print(f"Processing batch {i//batch_size + 1} with {len(batch)} accessions")

    # Submit mapping job
    submit_url = "https://rest.uniprot.org/idmapping/run"
    params = {'from': 'UniProtKB_AC-ID', 'to': 'UniProtKB', 'ids': ids}
    try:
        response = session.post(submit_url, data=params)
        response.raise_for_status()
        job_id = response.json()['jobId']
        print(f"âœ… Submitted job {job_id} for batch {batch_num}")
    except Exception as e:
        print(f"âŒ Submission failed for batch {batch_num}: {e}")
        continue

    # Poll job until finished
    status_url = f"https://rest.uniprot.org/idmapping/status/{job_id}"
    while True:
        status_response = session.get(status_url)
        status_response.raise_for_status()
        status_json = status_response.json()
        print(f"Checking status for job {job_id}: {status_json.get('jobStatus')}.")
        if status_json.get('jobStatus') is None:
            print(f"Job {job_id} finished. Fetching results")
            break
        if status_json.get('jobStatus') in ['FAILED', 'ERROR']:
            print(f"Job failed: {job_id}")
            break

        time.sleep(5)

    # Fetch results
    results_url = f"https://rest.uniprot.org/idmapping/uniprotkb/results/{job_id}?size=500"
    
    
    while results_url:
        results_response = session.get(results_url)
        results_response.raise_for_status()
        data = results_response.json()
        for result in data.get('results', []):
            acc = result['from']
            to_data = result.get('to')

            primary_acc = to_data.get("primaryAccession", acc)
            merged_to = primary_acc if primary_acc != acc else 'N/A'

            existence = result.get('to', {}).get('proteinExistence', 'Unknown')
            records.append({"Accession": acc, "PrimaryAcc" : primary_acc, "Existence": existence, "MergedTo": merged_to})
            save_counter += 1
            print(f"âœ… Processed {acc}: {existence}")

        for failed_acc in data.get('failedIds', []):
            records.append({"Accession": failed_acc, "PrimaryAcc" : "N/A", "Existence": "Not found", "MergedTo": "N/A"})
            save_counter += 1
            print(f"âŒ {failed_acc} not found in UniProt")
        
        results_url = results_response.links.get("next", {}).get("url")
    

    if save_counter >= 100:
        with open(checkpoint_file, "wb") as f:
            pickle.dump((i + 1, records), f)
        print(f"ðŸ’¾ Saved checkpoint at entry {i + 1} ({len(records)} total records).")
        save_counter = 0  # reset counter
    

    time.sleep(1)

# Save final results
existence_df = pd.DataFrame(records)
existence_df.to_csv("protein_existence_summary_isoforms_merged.csv", index=False)

existence_df = pd.read_csv("protein_existence_summary_isoforms_merged.csv")

existence_df["InLocalDB"] = existence_df["MergedTo"].apply(lambda x: x in uniprot_accessions_iso if pd.notna(x) else False)

merged_proteins_df = existence_df[existence_df["MergedTo"].notna()].copy()

count_merged = len(set(merged_proteins_df))
print(f"Total Merged/Retire Accessions Found in Uniprot: {count_merged}")

exist_1 = (existence_df["Existence"] == "1: Evidence at protein level").sum()
exist_2 = (existence_df["Existence"] == "2: Evidence at transcript level").sum()
exist_3 = (existence_df["Existence"] == "3: Inferred from homology").sum()
exist_4 = (existence_df["Existence"] == "4: Predicted").sum()
exist_5 = (existence_df["Existence"] == "5: Uncertain").sum()
exist_unknown = (existence_df["Existence"] == "Unknown").sum()
non_exist = (existence_df["Existence"] == "Not found").sum()

isoforms = existence_df["Accession"].str.contains("-").sum()

# Analysis of merged accessions
print("\nExistence Levels for Merged Accessions:")
exist_1_merged = (merged_proteins_df["Existence"] == "1: Evidence at protein level").sum()
exist_2_merged = (merged_proteins_df["Existence"] == "2: Evidence at transcript level").sum()
exist_3_merged = (merged_proteins_df["Existence"] == "3: Inferred from homology").sum()
exist_4_merged = (merged_proteins_df["Existence"] == "4: Predicted").sum()
exist_5_merged = (merged_proteins_df["Existence"] == "5: Uncertain").sum()
exist_unknown_merged = (merged_proteins_df["Existence"] == "Unknown").sum()
exist_notfound_merged = (merged_proteins_df["Existence"] == "Not found").sum()
print(f"Level 1: {exist_1_merged}")
print(f"Level 2: {exist_2_merged}")
print(f"Level 3: {exist_3_merged}")
print(f"Level 4: {exist_4_merged}")
print(f"Level 5: {exist_5_merged}")
print(f"Unknown: {exist_unknown_merged}")
print(f"Not Found: {exist_notfound_merged}")

# Check how many merged-to are in local DB
merged_proteins_df['InLocalDB'] = merged_proteins_df['MergedTo'].apply(lambda x: x in uniprot_accessions_iso)  # Use iso set for accuracy
in_local_count = merged_proteins_df['InLocalDB'].sum()

# merged_proteins_df.to_csv("merged_protein_existence_iso.csv")
print(f"\nMerged-To Accessions in Local Database: {in_local_count} / {count_merged}")

print(f"Existent proteins level 1: {exist_1}")
print(f"Existent proteins level 2: {exist_2}")
print(f"Existent proteins level 3: {exist_3}")
print(f"Existent proteins level 4: {exist_4}")
print(f"Existent proteins level 5: {exist_5}")
print(f"Existent proteins Unknown: {exist_unknown}")
print(f"Non-existent proteins: {non_exist}")
print(f"Number of isoforms: {isoforms}")


########################################################
############# Extract PTM features #####################
########################################################

accessions_fetch = existence_df[
    (existence_df["MergedTo"].isna())]["Accession"].tolist()

print(f"Number of accessions to fetch PTMs for: {len(accessions_fetch)}")


# Function to extract the PTM features

def fetch_uniprot_ptms(accessions, group_size = 100):
    session = requests_session_with_retries()

    unique_accessions = list(set(accessions))
    results = []

    groups = [unique_accessions[i:i + group_size] for i in range(0, len(unique_accessions), group_size)]

    print(f"Split into {len(groups)} groups for processing")
    
    for group_num, batch in enumerate(groups, 1):
        print(f"Processing group {group_num}/{len(groups)}")
        query = " OR ".join([f"accession:{acc}" for acc in batch])
        encoded_query = urllib.parse.quote(f"{query}")
        url = f"https://rest.uniprot.org/uniprotkb/search?query={encoded_query}&format=json&size=500"

        results_response = session.get(url)
        results_response.raise_for_status()
        data = results_response.json()

        for entry in data.get("results", []):
            acc = entry["primaryAccession"]
            features = entry.get("features", [])
            if features:
                for f in features:
                    results.append({
                        "Accession": acc,
                        "FeatureType": f.get("type", "N/A"),
                        "FeatureDescription": f.get("description", "N/A"),
                        "ECO": f.get("evidences", [])
                    })
            
            else:
                results.append({
                    "Accession": acc,
                    "FeatureType": "No feature",
                    "FeatureDescription": "N/A",
                    "ECO": []
                })
    
    return results

# Call the function
ptm_data = fetch_uniprot_ptms(accessions_fetch)
pd.DataFrame(ptm_data).to_csv("ptms_dbptm.csv", index=False)

